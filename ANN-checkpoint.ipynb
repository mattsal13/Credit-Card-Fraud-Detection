{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Keras. \n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"white\")\n",
    "\n",
    "## Define categorical funcion. This prepares entries for tensor analysis. \n",
    "\n",
    "def to_categorical(y):\n",
    "    return tf.keras.utils.to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_df = pd.read_csv(\"/Users/Matt/Documents/GitHub/Credit-Card-Fraud-Detection/creditcard.csv\")\n",
    "credit_card_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickout the valid transactions.\n",
    "\n",
    "valid_transactions = credit_card_df.loc[credit_card_df['Class'] == 0.0]\n",
    "fraudulent_transactions = credit_card_df.drop(valid_transactions.index)\n",
    "\n",
    "## For example, \n",
    "\n",
    "fraudulent_transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more balanced data set. This training set will have a ~ 75 / 25 split of valid and fraudulent transactions.\n",
    "# I'll randomly sample 1500 valid transactions and then add on the remaining 492 invalid transactions for a total of \n",
    "# 1992. \n",
    "\n",
    "valid_sample = valid_transactions.sample(1500)\n",
    "\n",
    "sample_data = pd.concat([valid_sample, fraudulent_transactions])\n",
    "\n",
    "# Set aside training/test data. \n",
    "\n",
    "sample_data_train = sample_data.sample(frac = .80, random_state = 440)\n",
    "sample_data_test = sample_data.drop(sample_data_train.index)\n",
    "\n",
    "# Make sure that the Class values (i.e. the entries of y) match the sample data.\n",
    "# The training data:\n",
    "\n",
    "X_train = np.array(sample_data_train.iloc[:, :-1])\n",
    "y_train = np.array(sample_data_train.iloc[:, -1])\n",
    "\n",
    "X_train = X_train.reshape(-1, 30)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# And the test data:\n",
    "\n",
    "X_test = np.array(sample_data_test.iloc[:, :-1])\n",
    "y_test = np.array(sample_data_test.iloc[:, -1])\n",
    "\n",
    "X_test = X_test.reshape(-1, 30)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further split the training data into train_train/validation. \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_train,X_val,y_train_train,y_val = train_test_split(X_train, y_train,\n",
    "                                                          test_size=.2,\n",
    "                                                          shuffle=True,\n",
    "                                                          stratify=y_train,\n",
    "                                                          random_state=440)\n",
    "\n",
    "# Check out the shape of the data. \n",
    "\n",
    "print(np.shape(X_train_train), type(X_train_train))\n",
    "print(np.shape(y_train_train), type(y_train_train))\n",
    "print(np.shape(X_val), type(X_val))\n",
    "print(np.shape(y_val), type(y_val))\n",
    "print('\\n\\n')\n",
    "print(np.shape(to_categorical(y_train_train)), type(to_categorical(y_train_train)))\n",
    "print(np.shape(to_categorical(y_val)), type(to_categorical(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Empty model\n",
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the layers. ONLY RUN THIS ONCE. Make sure the input_shape matches the number of features.\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(30,)))\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives a model summary\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we compile the model. We use binary cross-entropy. \n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the data. \n",
    "\n",
    "history = model.fit(X_train_train,\n",
    "                        to_categorical(y_train_train),\n",
    "                        epochs = 100,\n",
    "                        batch_size = 512,\n",
    "                        validation_data=(X_val,to_categorical(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the results. \n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's... intersting. \n",
    "\n",
    "set_style(\"whitegrid\")\n",
    "\n",
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "plt.scatter(range(1,101), history_dict['accuracy'], label = \"Training Accuracy\")\n",
    "plt.scatter(range(1,101), history_dict['val_accuracy'], label = \"Validation Set Accuracy\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Accuracy\", fontsize=18)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,6))\n",
    "\n",
    "plt.scatter(range(1,101), history_dict['loss'], label = \"Training Loss\")\n",
    "plt.scatter(range(1,101), history_dict['val_loss'], label = \"Validation Set Loss\")\n",
    "\n",
    "plt.xlabel(\"Epoch\", fontsize=18)\n",
    "plt.ylabel(\"Loss Function Value\", fontsize=18)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=18)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of test data. \n",
    "\n",
    "sample_data_test.value_counts('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction.\n",
    "\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_test.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
